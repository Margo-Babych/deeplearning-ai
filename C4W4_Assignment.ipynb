{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNVWL5h/IV5WiS6wBd692j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Margo-Babych/deeplearning-ai/blob/main/C4W4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 4: Using real world data\n",
        "Welcome! So far you have worked exclusively with generated data. This time you will be using the Daily Minimum Temperatures in Melbourne dataset which contains data of the daily minimum temperatures recorded in Melbourne from 1981 to 1990. In addition to be using Tensorflow's layers for processing sequence data such as Recurrent layers or LSTMs you will also use Convolutional layers to improve the model's performance.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "dlJ0wBwVezDF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g7SiwA2eU1Yu"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURES_CSV = './data/daily-min-temperatures.csv'\n",
        "\n",
        "with open(TEMPERATURES_CSV, 'r') as csvfile:\n",
        "    print(f\"Header looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"First data point looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"Second data point looks like this:\\n\\n{csvfile.readline()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "FZK8EOzqe4Ps",
        "outputId": "de7bf8fd-bcd5-4be8-e5f6-c3d2b875dd37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ea26dd73cd6b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEMPERATURES_CSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/daily-min-temperatures.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMPERATURES_CSV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Header looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First data point looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/daily-min-temperatures.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "kHavRlape7AU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data_from_file(filename):\n",
        "\n",
        "    times = []\n",
        "    temperatures = []\n",
        "\n",
        "    with open(filename) as csvfile:\n",
        "\n",
        "        ### START CODE HERE\n",
        "\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        # Skip the first line\n",
        "        next(reader)\n",
        "\n",
        "        # Initialize a counter for the time steps (starting at zero)\n",
        "        time_step = 0\n",
        "\n",
        "        # Iterate through the rows in the CSV file\n",
        "        for row in reader:\n",
        "            # Extract the temperature (as a float) from the second column (index 1)\n",
        "            temperature = float(row[1])\n",
        "\n",
        "            # Append the time step and temperature to their respective lists\n",
        "            times.append(time_step)\n",
        "            temperatures.append(temperature)\n",
        "\n",
        "            # Increment the time step\n",
        "            time_step += 1\n",
        "\n",
        "        ### END CODE HERE\n",
        "\n",
        "    return times, temperatures"
      ],
      "metadata": {
        "id": "nH48VbpXe_ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function and save all \"global\" variables within the G class (G stands for global)\n",
        "@dataclass\n",
        "class G:\n",
        "    TEMPERATURES_CSV = './data/daily-min-temperatures.csv'\n",
        "    times, temperatures = parse_data_from_file(TEMPERATURES_CSV)\n",
        "    TIME = np.array(times)\n",
        "    SERIES = np.array(temperatures)\n",
        "    SPLIT_TIME = 2500\n",
        "    WINDOW_SIZE = 64\n",
        "    BATCH_SIZE = 32\n",
        "    SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(G.TIME, G.SERIES)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AdwR-clJfAW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(time, series, time_step=G.SPLIT_TIME):\n",
        "\n",
        "    time_train = time[:time_step]\n",
        "    series_train = series[:time_step]\n",
        "    time_valid = time[time_step:]\n",
        "    series_valid = series[time_step:]\n",
        "\n",
        "    return time_train, series_train, time_valid, series_valid\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "time_train, series_train, time_valid, series_valid = train_val_split(G.TIME, G.SERIES)"
      ],
      "metadata": {
        "id": "28wt6VokfDIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "    ds = ds.batch(batch_size).prefetch(1)\n",
        "    return ds\n",
        "\n",
        "\n",
        "# Apply the transformation to the training set\n",
        "train_set = windowed_dataset(series_train, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE, shuffle_buffer=G.SHUFFLE_BUFFER_SIZE)"
      ],
      "metadata": {
        "id": "A-oeS1jwfFsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_uncompiled_model():\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
        "                      strides=1,\n",
        "                      activation=\"relu\",\n",
        "                      padding='causal',\n",
        "                      input_shape=[None, 1]),\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "        tf.keras.layers.LSTM(64),\n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "wvf0oHKefINt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your uncompiled model\n",
        "\n",
        "# Create an instance of the model\n",
        "uncompiled_model = create_uncompiled_model()\n",
        "\n",
        "# Get one batch of the training set(X = input, y = label)\n",
        "for X, y in train_set.take(1):\n",
        "\n",
        "    # Generate a prediction\n",
        "    print(f'Testing model prediction with input of shape {X.shape}...')\n",
        "    y_pred = uncompiled_model.predict(X)\n",
        "\n",
        "# Compare the shape of the prediction and the label y (remove dimensions of size 1)\n",
        "y_pred_shape = y_pred.squeeze().shape\n",
        "\n",
        "assert y_pred_shape == y.shape, (f'Squeezed predicted y shape = {y_pred_shape} '\n",
        "                                           f'whereas actual y shape = {y.shape}.')\n",
        "\n",
        "print(\"Your current architecture is compatible with the windowed dataset! :)\")"
      ],
      "metadata": {
        "id": "fSyYb7akfK0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(dataset):\n",
        "\n",
        "    model = create_uncompiled_model()\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch / 20))\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    # Select your optimizer\n",
        "    optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
        "\n",
        "    # Compile the model passing in the appropriate loss\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=[\"mae\"])\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "7jex1xqgfM_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training with dynamic LR\n",
        "lr_history = adjust_learning_rate(train_set)"
      ],
      "metadata": {
        "id": "Y6hIIz68fQRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.semilogx(lr_history.history[\"lr\"], lr_history.history[\"loss\"])\n",
        "plt.axis([1e-4, 10, 0, 10])"
      ],
      "metadata": {
        "id": "PbPjhA6TfSSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "\n",
        "    model = create_uncompiled_model()\n",
        "\n",
        "    ### START CODE HERE\n",
        "\n",
        "    model.compile(loss='mae',\n",
        "                  optimizer='adam',\n",
        "                  metrics=[\"mae\"])\n",
        "\n",
        "\n",
        "    ### END CODE HERE\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "b1S5xUSFfUhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save an instance of the model\n",
        "model = create_model()\n",
        "\n",
        "# Train it\n",
        "history = model.fit(train_set, epochs=50)"
      ],
      "metadata": {
        "id": "iyCClBS0fXN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(true_series, forecast):\n",
        "\n",
        "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()\n",
        "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
        "\n",
        "    return mse, mae"
      ],
      "metadata": {
        "id": "EhTwNCAyfZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ],
      "metadata": {
        "id": "YQoYT7ssfbmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the forecast for all the series\n",
        "rnn_forecast = model_forecast(model, G.SERIES, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the validation set\n",
        "rnn_forecast = rnn_forecast[G.SPLIT_TIME - G.WINDOW_SIZE:-1]\n",
        "\n",
        "# Plot the forecast\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(time_valid, series_valid)\n",
        "plot_series(time_valid, rnn_forecast)"
      ],
      "metadata": {
        "id": "d7y1ny0rfd-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse, mae = compute_metrics(series_valid, rnn_forecast)\n",
        "\n",
        "print(f\"mse: {mse:.2f}, mae: {mae:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "89crG8affh2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your model in the SavedModel format\n",
        "model.save('saved_model/my_model')\n",
        "\n",
        "# Compress the directory using tar\n",
        "! tar -czvf saved_model.tar.gz saved_model/"
      ],
      "metadata": {
        "id": "zf11iDrgfj4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}