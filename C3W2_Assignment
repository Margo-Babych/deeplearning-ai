Week 2: Diving deeper into the BBC News archive
Welcome! In this assignment you will be revisiting the BBC News Classification Dataset, which contains 2225 examples of news articles with their respective labels.

This time you will not only work with the tokenization process but you will also create a classifier using specialized layers for text data such as Embedding and GlobalAveragePooling1D.

Let's get started!

# IMPORTANT: This will check your notebook's metadata for grading.
# Please do not continue the lab unless the output of this cell tells you to proceed. 
!python add_metadata.py --filename C3W2_Assignment.ipynb

# grader-required-cell

import io
import csv
import tensorflow as tf
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# grader-required-cell

with open("./bbc-text.csv", 'r') as csvfile:
    print(f"First line (header) looks like this:\n\n{csvfile.readline()}")
    print(f"Each data point looks like this:\n\n{csvfile.readline()}")

As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article.

Defining useful global variables
Next, you will define some global variables that will be used in the unit tests after your solutions. Please do not use these in the function body of the graded functions.

NUM_WORDS: The maximum number of words to keep, based on word frequency. Defaults to 1000.
EMBEDDING_DIM: Dimension of the dense embedding, will be used in the embedding layer of the model. Defaults to 16.
MAXLEN: Maximum length of all sequences. Defaults to 120.
PADDING: Padding strategy (pad either before or after each sequence.). Defaults to 'post'.
OOV_TOKEN: Token to replace out-of-vocabulary words during text_to_sequence calls. Defaults to "<OOV>".
TRAINING_SPLIT: Proportion of data used for training. Defaults to 0.8
For now leave them unchanged but after submitting your assignment for grading you are encouraged to come back here and play with these parameters to see the impact they have in the classification process

# grader-required-cell

NUM_WORDS = 1000
EMBEDDING_DIM = 16
MAXLEN = 120
PADDING = 'post'
OOV_TOKEN = "<OOV>"
TRAINING_SPLIT = .8
# grader-required-cell

# grader-required-cell

NUM_WORDS = 1000
EMBEDDING_DIM = 16
MAXLEN = 120
PADDING = 'post'
OOV_TOKEN = "<OOV>"
TRAINING_SPLIT = .8

# grader-required-cell

def remove_stopwords(sentence):
    """
    Removes a list of stopwords
    
    Args:
        sentence (string): sentence to remove the stopwords from
    
    Returns:
        sentence (string): lowercase sentence without the stopwords
    """
    # List of stopwords
    stopwords = ["a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having", "he", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves" ]
    
    # Sentence converted to lowercase-only
    sentence = sentence.lower()

    words = sentence.split()
    no_words = [w for w in words if w not in stopwords]
    sentence = " ".join(no_words)

    return sentence


def parse_data_from_file(filename):
    """
    Extracts sentences and labels from a CSV file
    
    Args:
        filename (string): path to the CSV file
    
    Returns:
        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels
    """
    sentences = []
    labels = []
    with open(filename, 'r') as csvfile:
        reader = csv.reader(csvfile, delimiter=',')
        next(reader)
        for row in reader:
            labels.append(row[0])
            sentence = row[1]
            sentence = remove_stopwords(sentence)
            sentences.append(sentence)

    return sentences, labels

# grader-required-cell

# Test the functions
sentences, labels = parse_data_from_file("./bbc-text.csv")

print(f"There are {len(sentences)} sentences in the dataset.\n")
print(f"First sentence has {len(sentences[0].split())} words (after removing stopwords).\n")
print(f"There are {len(labels)} labels in the dataset.\n")
print(f"The first 5 labels are {labels[:5]}")

# grader-required-cell

# GRADED FUNCTIONS: train_val_split
def train_val_split(sentences, labels, training_split):
    """
    Splits the dataset into training and validation sets
    
    Args:
        sentences (list of string): lower-cased sentences without stopwords
        labels (list of string): list of labels
        training split (float): proportion of the dataset to convert to include in the train set
    
    Returns:
        train_sentences, validation_sentences, train_labels, validation_labels - lists containing the data splits
    """
    
    ### START CODE HERE
    
    # Compute the number of sentences that will be used for training (should be an integer)
    train_size = int(len(sentences) * training_split)

    # Split the sentences and labels into train/validation splits
    train_sentences = sentences[:train_size]
    train_labels = labels[:train_size]

    validation_sentences = sentences[train_size:]
    validation_labels =  labels[train_size:]
    
    ### END CODE HERE
    
    return train_sentences, validation_sentences, train_labels, validation_labels

# grader-required-cell

# Test your function
train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)

print(f"There are {len(train_sentences)} sentences for training.\n")
print(f"There are {len(train_labels)} labels for training.\n")
print(f"There are {len(val_sentences)} sentences for validation.\n")
print(f"There are {len(val_labels)} labels for validation.")

# grader-required-cell

# GRADED FUNCTION: fit_tokenizer
def fit_tokenizer(train_sentences, num_words, oov_token):
    """
    Instantiates the Tokenizer class on the training sentences
    
    Args:
        train_sentences (list of string): lower-cased sentences without stopwords to be used for training
        num_words (int) - number of words to keep when tokenizing
        oov_token (string) - symbol for the out-of-vocabulary token
    
    Returns:
        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary
    """
    
    ### START CODE HERE
    
    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token
    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)

# Generate the word index dictionary for the training sentences
    tokenizer.fit_on_texts(train_sentences)
# Generate the training sequences

    
    # Fit the tokenizer to the training sentences
    
    
    ### END CODE HERE
    
    return tokenizer

# grader-required-cell

# Test your function
tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)
word_index = tokenizer.word_index

print(f"Vocabulary contains {len(word_index)} words\n")
print("<OOV> token included in vocabulary" if "<OOV>" in word_index else "<OOV> token NOT included in vocabulary")

# grader-required-cell

# GRADED FUNCTION: seq_and_pad
def seq_and_pad(sentences, tokenizer, padding, maxlen):
    """
    Generates an array of token sequences and pads them to the same length
    
    Args:
        sentences (list of string): list of sentences to tokenize and pad
        tokenizer (object): Tokenizer instance containing the word-index dictionary
        padding (string): type of padding to use
        maxlen (int): maximum length of the token sequence
    
    Returns:
        padded_sequences (array of int): tokenized sentences padded to the same length
    """ 
    
    ### START CODE HERE
       
    # Convert sentences to sequences
    sequences =tokenizer.texts_to_sequences(sentences)
  
    
    # Pad the sequences using the correct padding and maxlen
    padded_sequences = pad_sequences(sequences, padding=padding, maxlen=maxlen)
   
    
    ### END CODE HERE
    
    return padded_sequences

# grader-required-cell

# Test your function
train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)
val_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)

print(f"Padded training sequences have shape: {train_padded_seq.shape}\n")
print(f"Padded validation sequences have shape: {val_padded_seq.shape}")

# grader-required-cell

# GRADED FUNCTION: tokenize_labels
def tokenize_labels(all_labels, split_labels):
    """
    Tokenizes the labels
    
    Args:
        all_labels (list of string): labels to generate the word-index from
        split_labels (list of string): labels to tokenize
    
    Returns:
        label_seq_np (array of int): tokenized labels
    """
    
    ### START CODE HERE
    
    # Instantiate the Tokenizer (no additional arguments needed)
    label_tokenizer = label_tokenizer = Tokenizer()
    
    # Fit the tokenizer on all the labels
    label_tokenizer.fit_on_texts(all_labels)
    
    # Convert labels to sequences
    label_seq = label_tokenizer.texts_to_sequences(split_labels)
    
    # Convert sequences to a numpy array. Don't forget to subtract 1 from every entry in the array!
    label_seq_np = np.array(label_seq) - 1
    
    
    ### END CODE HERE
    
    return label_seq_np

# grader-required-cell

# Test your function
train_label_seq = tokenize_labels(labels, train_labels)
val_label_seq = tokenize_labels(labels, val_labels)

print(f"First 5 labels of the training set should look like this:\n{train_label_seq[:5]}\n")
print(f"First 5 labels of the validation set should look like this:\n{val_label_seq[:5]}\n")
print(f"Tokenized labels of the training set have shape: {train_label_seq.shape}\n")
print(f"Tokenized labels of the validation set have shape: {val_label_seq.shape}\n")

# grader-required-cell

# GRADED FUNCTION: create_model
def create_model(num_words, embedding_dim, maxlen):
    """
    Creates a text classifier model
    
    Args:
        num_words (int): size of the vocabulary for the Embedding layer input
        embedding_dim (int): dimensionality of the Embedding layer output
        maxlen (int): length of the input sequences
    
    Returns:
        model (tf.keras Model): the text classifier model
    """
    
    tf.random.set_seed(123)
    
    ### START CODE HERE
    from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense

    model =tf.keras.Sequential([ 
        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),
        tf.keras.layers.GlobalAveragePooling1D(),
        tf.keras.layers.Dense(24, activation='relu'),
        tf.keras.layers.Dense(5, activation='softmax')
    ])
   
    

    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy']) 


    ### END CODE HERE

    return model

# NOTE: Please do not edit this cell

model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)

history = model.fit(train_padded_seq, train_label_seq, epochs=30, validation_data=(val_padded_seq, val_label_seq))

Epoch 1/30
56/56 [==============================] - 1s 4ms/step - loss: 1.5988 - accuracy: 0.2826 - val_loss: 1.5817 - val_accuracy: 0.4539
Epoch 2/30
56/56 [==============================] - 0s 2ms/step - loss: 1.5505 - accuracy: 0.4393 - val_loss: 1.5072 - val_accuracy: 0.4449
Epoch 3/30
56/56 [==============================] - 0s 2ms/step - loss: 1.4295 - accuracy: 0.5129 - val_loss: 1.3490 - val_accuracy: 0.5461
Epoch 4/30
56/56 [==============================] - 0s 2ms/step - loss: 1.2200 - accuracy: 0.6320 - val_loss: 1.1292 - val_accuracy: 0.6899
Epoch 5/30
56/56 [==============================] - 0s 2ms/step - loss: 0.9706 - accuracy: 0.7820 - val_loss: 0.9012 - val_accuracy: 0.7798
Epoch 6/30
56/56 [==============================] - 0s 2ms/step - loss: 0.7353 - accuracy: 0.8742 - val_loss: 0.7046 - val_accuracy: 0.8697
Epoch 7/30
56/56 [==============================] - 0s 2ms/step - loss: 0.5476 - accuracy: 0.9230 - val_loss: 0.5573 - val_accuracy: 0.8787
Epoch 8/30
56/56 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.9376 - val_loss: 0.4596 - val_accuracy: 0.8899
Epoch 9/30
56/56 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.9472 - val_loss: 0.3969 - val_accuracy: 0.8989
Epoch 10/30
56/56 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.9590 - val_loss: 0.3508 - val_accuracy: 0.9056
Epoch 11/30
56/56 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9635 - val_loss: 0.3196 - val_accuracy: 0.9079
Epoch 12/30
56/56 [==============================] - 0s 2ms/step - loss: 0.1889 - accuracy: 0.9674 - val_loss: 0.2965 - val_accuracy: 0.9146
Epoch 13/30
56/56 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9708 - val_loss: 0.2803 - val_accuracy: 0.9146
Epoch 14/30
56/56 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9764 - val_loss: 0.2670 - val_accuracy: 0.9191
Epoch 15/30
56/56 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9792 - val_loss: 0.2551 - val_accuracy: 0.9169
Epoch 16/30
56/56 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9815 - val_loss: 0.2460 - val_accuracy: 0.9236
Epoch 17/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0981 - accuracy: 0.9843 - val_loss: 0.2397 - val_accuracy: 0.9191
Epoch 18/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9876 - val_loss: 0.2345 - val_accuracy: 0.9303
Epoch 19/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9916 - val_loss: 0.2295 - val_accuracy: 0.9213
Epoch 20/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9921 - val_loss: 0.2257 - val_accuracy: 0.9213
Epoch 21/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9944 - val_loss: 0.2253 - val_accuracy: 0.9236
Epoch 22/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9949 - val_loss: 0.2205 - val_accuracy: 0.9258
Epoch 23/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9972 - val_loss: 0.2209 - val_accuracy: 0.9281
Epoch 24/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9978 - val_loss: 0.2175 - val_accuracy: 0.9213
Epoch 25/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0402 - accuracy: 0.9978 - val_loss: 0.2179 - val_accuracy: 0.9281
Epoch 26/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.9983 - val_loss: 0.2171 - val_accuracy: 0.9236
Epoch 27/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9989 - val_loss: 0.2180 - val_accuracy: 0.9236
Epoch 28/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9989 - val_loss: 0.2161 - val_accuracy: 0.9191
Epoch 29/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9994 - val_loss: 0.2176 - val_accuracy: 0.9213
Epoch 30/30
56/56 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.2170 - val_accuracy: 0.9213

def plot_graphs(history, metric):
    plt.plot(history.history[metric])
    plt.plot(history.history[f'val_{metric}'])
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend([metric, f'val_{metric}'])
    plt.show()
    
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")
# Reverse word index
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

# Save the embedding layer
e = model.layers[0]

# Save the weights of the embedding layer
weights = e.get_weights()[0]
print(f"Weights of embedding layer have shape: {weights.shape}")

# Generate files for embedding visualization
out_v = io.open('vecs.tsv', 'w', encoding='utf-8')
out_m = io.open('meta.tsv', 'w', encoding='utf-8')
for word_num in range(1, NUM_WORDS):
    word = reverse_word_index[word_num]
    embeddings = weights[word_num]
    out_m.write(word + "\n")
    out_v.write('\t'.join([str(x) for x in embeddings]) + "\n")
out_v.close()
out_m.close()

